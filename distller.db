{"What is the training data used for each experimemnt with dataset sizes?": " For the English-to-German translation task, the big transformer model used the WMT 2014 English-to-German dataset. For the English-to-French translation task, the big model used the WMT 2014 English-to-French dataset. For English constituency parsing, the model used the Wall Street Journal (WSJ) portion of the Penn Treebank and the larger high-confidence and BerkleyParser corpora.", "What is the main innovation or new idea in the paper?": " The main innovation or new idea in the paper is the Transformer, a new simple network architecture based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.", "What is the model architecture?": " The Transformer follows an encoder-decoder structure with a stack of 6 identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. The encoder maps an input sequence of symbol representations to a sequence of continuous representations, and the decoder then generates an output sequence of symbols one element at a time.", "How many tokens or examples are in the training set?": " Approximately 40,000 training sentences and 16,000 tokens for the WSJ only setting and approximately 17M sentences and 32,000 tokens for the semi-supervised setting.", "Where is the training set scraped from or obtained and what modalities does it include?": " The training set is from the Wall Street Journal (WSJ) training set of 40K sentences and includes text.", "What are the tasks performed by the model?": " The Transformer model has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations. It has also been used for simple-language question answering and language modeling tasks.", "How is evaluation performed?": " Evaluation is performed by measuring the change in performance on English-to-German translation on the development set, newstest2013, using beam search and no checkpoint averaging. For English constituency parsing, evaluation is performed by measuring the Training WSJ 23 F1 score on Section 23 of WSJ.", "What is the model architecture and what prior work used simnilar architecture?": " The Transformer model architecture uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. Prior work such as the Extended Neural GPU, ByteNet, and ConvS2S have used similar architectures, but the Transformer is the first model to rely entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.", "What is the main innovation or new idea in the paper?-attention_is_all_you_need": " The main innovation or new idea in the paper is the Transformer, a new simple network architecture based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.", "How many tokens or examples are in the training set?-attention_is_all_you_need": " Approximately 40,000 tokens and 17 million sentences.", "Where is the training set scraped from or obtained and what modalities does it include?-attention_is_all_you_need": " The training set is obtained from the Penn Treebank and includes text modalities.", "What are the tasks performed by the model?-attention_is_all_you_need": " The Transformer model is used for tasks such as language modeling, machine translation, reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.", "How is evaluation performed?-attention_is_all_you_need": " Evaluation is performed by measuring the change in performance on English-to-German translation on the development set, newstest2013. Beam search is used, but no checkpoint averaging. Results are presented in Table 3. For English constituency parsing, evaluation is performed by selecting the dropout, both attention and residual, learning rates and beam size on the Section 22 development set, and measuring the results on Section 23 of WSJ.", "What is the model architecture and what prior work used simnilar architecture?-attention_is_all_you_need": " The model architecture is composed of a stack of N= 6 identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. This architecture is similar to the Extended Neural GPU, ByteNet, and ConvS2S, which all use convolutional neural networks as basic building blocks to compute hidden representations in parallel for all input and output positions.", "What is the main innovation or new idea in the paper?-bert": " The main innovation or new idea in the paper is the use of a bidirectional Transformer for pre-training, which is different from the left-to-right Transformer used in OpenAI GPT and the concatenation of independently trained left-to-right and right-to-left LSTMs used in ELMo.", "How many tokens or examples are in the training set?-bert": " I don't know.", "Where is the training set scraped from or obtained and what modalities does it include?-bert": " The training set for BERT is obtained from a variety of sources, including Wikipedia, BookCorpus, and OpenWebText. It includes text modalities.", "What are the tasks performed by the model?-bert": " The model is used to perform natural language understanding tasks on the GLUE benchmark, question answering on the SQuAD v1.1 dataset, and named entity recognition on the CoNLL-2003 dataset.", "How is evaluation performed?-bert": " To fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights W2RKH, where K is the number of labels. We compute a standard classification loss with CandW, i.e., log(softmax(CWT)).\n\nThe evaluation for GLUE is performed by computing a standard classification loss with CandW, i.e., log(softmax(CWT)).", "What is the model architecture and what prior work used simnilar architecture?-bert": " BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-left LSTMs to generate features for downstream tasks.", "What is the main innovation or new idea in the paper?-whisper": " The main innovation or new idea in the paper is the development of Whisper, a robust speech recognition system that is trained on a large-scale weakly supervised dataset and is able to transfer between both text and speech language tasks, demonstrating transfer between them.", "How many tokens or examples are in the training set?-whisper": " The training set contains 680k hours of data.", "Where is the training set scraped from or obtained and what modalities does it include?-whisper": " The training set is scraped from the internet and includes audio paired with transcripts, as well as audio language detection, X!en translation data, and voice activity detection.", "What are the tasks performed by the model?-whisper": " The model performs language tasks, such as transcription and translation, as well as voice activity detection, speaker diarization, and language identification.", "How is evaluation performed?-whisper": " Evaluation is typically performed using the word error rate (WER) metric, which is based on string edit distance and penalizes all differences between the model's output and the reference transcript including innocuous differences in transcript style. To address this problem, extensive standardization of text before the WER calculation is used to minimize penalization of non-semantic differences.", "What is the model architecture and what prior work used simnilar architecture?-whisper": " The model architecture is an audio conditional language model and similar architectures have been used in works such as Narayanan et al. (2018), Likhomanenko et al. (2020), and Chan et al. (2021).", "What is the main innovation or new idea in the paper?-gpt3": " The main innovation or new idea in the paper is the investigation of how good humans are at detecting longer news articles generated by GPT-3 175B.", "How many tokens or examples are in the training set?-gpt3": " I don't know.", "Where is the training set scraped from or obtained and what modalities does it include?-gpt3": " The training set for GPT-3 is a combination of public datasets, including text from books, websites, and other sources. It includes natural language text.", "What are the tasks performed by the model?-gpt3": " The model performs tasks such as code and writing auto-completion, grammar assistance, game narrative generation, improving search engine responses, and answering questions.", "How is evaluation performed?-gpt3": " Human evaluation experiments are conducted to evaluate the model.", "What is the model architecture and what prior work used simnilar architecture?-gpt3": " The model architecture used is the same as GPT-2 [RWC+19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that they use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. Other prior work that used similar architecture includes the mixture-of-experts method [SMM+17], which produced 100 billion parameter models and 50 billion parameter translation models [AJF19], and the conditional computation framework [BLC13]."}